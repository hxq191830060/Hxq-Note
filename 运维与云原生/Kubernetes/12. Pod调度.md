调度就是将Pod放到合适的Node上

* 首先要满足Pod的资源要求
* 其次要满足pod的一些特殊关系的要求
* 最次要满足node的一些限制条件的要求
* 最后要做到整个集群资源和合理利用

Kubernetes提供调度能力

* 资源调度
* 关系调度



# 1. 资源调度

* resources.requests/limit——对资源的要求

* Qos——Quality Of Service，Kubernetes用来表达一个Pod在资源能力上的服务治理的标准，有三种Qos Class
  * **Guaranteed**：为一些需要资源保障能力的Pod进行配置
  * **Burstable**：为一些希望有弹性能力的Pod进行配置
  * **BestEffort**：尽力而为的质量服务
* 用户没办法指定Pod属于哪一类Qos，而是通过requests和limit组合来映射上Qos Class
  * Guaranteed Pod——基础资源（CPU和内存），必须让requests==limit
  * Burstable Pod——CPU/Memory的requests和limit不相等
  * BestEffort——所有资源的requests和limit都不填

* 如果我们开启了kubelet的cpu-manager-policy=static特性，那么如果Guaranteed Qos的request是一个整数n，那么会将n个CPU绑定到该Guaranteed Pod上，request非整数的Guaranteed，Burstable，BestEffort，他们requests的CPU会组成一个CPU Share Pool，根据它们根据不同的权重划分时间片来使用
* Memory按照不同的Qos划分OOMScore，比如Guaranteed，配置-998的OOMScore，Burstable根据内存设计的大小和节点关系分配2-999的OOMScore，BestEffort固定分配1000的OOMScore，OOMScore越高，在物理机出现OOM时优先被kill掉
* 需要驱逐pod时，优先考虑驱逐BestEffort的Pod



## 1.1 资源Quota

* 限制每个namespace的资源用量
* scope
  * Terminating/NotTerminating
  * BestEffort/NotBestEffort
  * PriorityClass

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: demo-quota
  namespace: demo-ns #作用的命名空间
spec:
  hard: #资源限制
    cpu: "1000"
    memory: 200Gi
    pods: "10"
  scopeSelector: #选择
    matchExpressions:
    - operator: Exists
      scopeName: NotBestEffort
#限制命名空间demo-ns下，非BestEffort Qos能够使用的资源
#CPU只能使用1000个
#内存只能使用200G
#pods只能创建10个
```





# 2. 关系调度

## 2.1 定向调度

在Pod上声明 **nodeName**或**nodeSelector**——>**强制调度**(即使指定的Node不存在，也会执行调度，但是会报错)

### 2.1.1 nodeName

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-nodename
  namespace: dev
spec:
  containers:
  - name: nginx
    image: nginx:1.17.1
  nodeName: node1 # 指定调度到node1节点上
```

### 2.1.2 nodeSelector

为Node添加标签

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-nodeselector
  namespace: dev
spec:
  containers:
  - name: nginx
    image: nginx:1.17.1
  nodeSelector: 
    nodeenv: pro # 指定调度到具有nodeenv=pro标签的节点上
```



## 2.2 亲和性调度

**亲和性**：如果两个应用频繁交互，那么有必要利用亲和性让两个应用尽可能的靠近，减少网络IO

**反亲和性**：当应用采用多副本部署时，有必要采用反亲和性将各个应用实例分散到各个node上，提高服务的可用性



**亲和性调度**分为三类

* **nodeAffinity(node亲和性)**

  以node为目标，让pod尽可能的与node亲和

* **podAffinity(pod亲和性)**

  以pod为目标，让pod之间尽可能亲和

* **podAntiAffinity(pod反亲和性)**

  以pod为目标，让pod之间尽可能反亲和



### 2.2.1 node亲和性

* 如果同时定义了nodeSelector和nodeAffinity，那么必须两个条件都得到满足，Pod才能运行在指定的Node上
* 如果nodeAffinity指定了多个nodeSelectorTerms，那么只需要其中一个能够匹配成功即可
* 如果一个nodeSelectorTerms中有多个matchExpressions ，则一个节点必须满足所有的才能匹配成功

```yaml
pod:
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIngnoredDuringExecution: #Node节点必须满足的条件，不满足的Node不考虑
          nodeSelectorTerms: #node选择列表
            matchFields: #要求node拥有这些field
            matchExpressions: #要求node拥有如下标签
              key: #键
              value: #值
              operator: #操作符
        preferredDuringSchedulingIgnoredDuringExecution: #优先调度到满足指令规则的Node上(倾向)
          preference: 
            matchFields: 
            matchExpressions: 
              key: #键
              value: #值
              operator: #操作符
          weight: #倾向权重
```

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-nodeaffinity-required
  namespace: dev
spec:
  containers:
  - name: nginx
    image: nginx:1.17.1
  affinity:  #亲和性设置
    nodeAffinity: #设置node亲和性
      requiredDuringSchedulingIgnoredDuringExecution: # 硬限制
        nodeSelectorTerms:
        - matchExpressions: # 匹配env的值在["xxx","yyy"]中的标签
          - key: nodeenv
            operator: In
            values: ["xxx","yyy"]
```

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-nodeaffinity-preferred
  namespace: dev
spec:
  containers:
  - name: nginx
    image: nginx:1.17.1
  affinity:  #亲和性设置
    nodeAffinity: #设置node亲和性
      preferredDuringSchedulingIgnoredDuringExecution: # 软限制
      - weight: 1
        preference:
          matchExpressions: # 匹配env的值在["xxx","yyy"]中的标签(当前环境没有)
          - key: nodeenv
            operator: In
            values: ["xxx","yyy"]
```



### 2.2.2 Pod亲和性

```yaml
pod:
  spec:
    affinity:
      podAffinity: 
        requiredDuringSchedulingIgnoredDuringExecution: #必须和某些Pod一起调度
          namespace: #指定参照Pod的namespace
          topologyKey: #用于指定调度作用域,如果指定为kubernetes.io/hostname,那么以Node节点为区分范围;如果指定为beta.kubernetes.io/os,以Node节点的操作系统类型来区分
          labelSelector: #标签选择器
            matchExpressions: 
              key: #键
              value: #值
              operator: #操作符
            matchLabels: 
        preferredDuringSchedulingIgnoredDuringExecution: #优先与某些Pod一起调度
          #同上
          weight: #权重
```

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-podaffinity-required
  namespace: dev
spec:
  containers:
  - name: nginx
    image: nginx:1.17.1
  affinity:  #亲和性设置
    podAffinity: #设置pod亲和性
      requiredDuringSchedulingIgnoredDuringExecution: # 硬限制
      - labelSelector:
          matchExpressions: # 匹配env的值在["xxx","yyy"]中的标签
          - key: podenv
            operator: In
            values: ["xxx","yyy"]
        topologyKey: kubernetes.io/hostname
#新Pod必须要与拥有标签nodeenv=xxx或者nodeenv=yyy的pod在同一Node上
```



### 2.2.3 Pod反亲和性

属性同PodAffinity

* requiredDuringSchedulingIgnoredDuringExecution：禁止和某些Pod一起调度
* preferredDuringSchedulingIgnoredDuringExecution：优先不和某些Pod一起调度

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-podantiaffinity-required
  namespace: dev
spec:
  containers:
  - name: nginx
    image: nginx:1.17.1
  affinity:  #亲和性设置
    podAntiAffinity: #设置pod亲和性
      requiredDuringSchedulingIgnoredDuringExecution: # 硬限制
      - labelSelector:
          matchExpressions: # 匹配podenv的值在["pro"]中的标签
          - key: podenv
            operator: In
            values: ["pro"]
        topologyKey: kubernetes.io/hostname
#新Pod必须要与拥有标签nodeenv=pro的pod不在同一Node上
```

## 2.3 污点(容忍)调度

我们可以给Node添加**污点**，这样Node与Pod会有一些**互斥行为**

>key=value:effect
>
>value是污点标签，effect是描述污点作用，有以下的值
>
>- PreferNoSchedule：kubernetes将尽量避免把Pod调度到具有该污点的Node上，除非没有其他节点可调度
>- NoSchedule：kubernetes将不会把Pod调度到具有该污点的Node上，但不会影响当前Node上已存在的Pod
>- NoExecute：kubernetes将不会把Pod调度到具有该污点的Node上，同时也会将Node上已存在的Pod驱离

### 2.3.1 设置污点和去除污点

```shell
# 设置污点
kubectl taint nodes node1 key=value:effect

# 去除污点
kubectl taint nodes node1 key:effect-

# 去除所有污点
kubectl taint nodes node1 key-
```



### 2.3.2 容忍

Node通过**污点**拒绝pod调度上去，Pod通过**容忍**忽略拒绝

**容忍属性**

```yaml
pod:
  spec:
    tolerations:
      key: #要容忍的污点的key
      value: #要容忍的污点的value
      operator: #key和value的操作符
      effect: #对应污点的effect,空代表匹配所有影响
      tolerationSeconds: #容忍时间,当effect为NoExecute时生效，表示pod在Node上的停留时间
```

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-toleration
  namespace: dev
spec:
  containers:
  - name: nginx
    image: nginx:1.17.1
  tolerations:      # 添加容忍
  - key: "tag"        # 要容忍的污点的key
    operator: "Equal" # 操作符
    value: "heima"    # 容忍的污点的value
    effect: "NoExecute"   # 添加容忍的规则，这里必须和标记的污点规则相同
```



# 3. 高级调度—优先级调度

* 如果集群资源不够，如何做到集群的合理利用
  * 先到先得策略
  * **优先级策略**

* 优先级调度——比如说有一个 Node 已经被一个 Pod 占用了，这个 Node 只有 2 个 CPU。另一个高优先级 Pod 来的时候，低优先级的 Pod 应该把这两个 CPU 让给高优先级的 Pod 去使用。低优先级的 Pod 需要回到等待队列，或者是业务重新提交

* 优先级调度需要一个PriorityClass，然后为每个Pod配置上不同的PriotiryClassName

```yml
#创建一个名为high的PriorityClass，得分为10000
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high
value: 10000
globalDefault: false
---
#创建一个名为low的PriorityClass，得分为100
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: low
value: 100
globalDefault: false
---
#每个Pod配置上PriorityClassName
spec:
  priorityClassName: high
```

* Kubernetes里面内置了默认优先级DefaultPriorityWhenNoDefaultClassExists=0（如果Pod没有配置优先级，那么默认为0）
  * 用户可配置的最大优先级限制HighestuserDefinablePriority=1000000000
  * 系统级别优先级SystemCriticalPriority=2000000000
  * 内置系统级别优先级
    * system-cluster-critical
    * system-node-critical



## 优先级调度过程

* Pod1配置了高优先级，Pod2配置了低优先级，同时提交Pod1和Pod2到调度队列
* 调度器处理队列的时候会挑选一个高优先级的 Pod1 进行调度，经过调度过程把 Pod1 绑定到 Node1 上
* 其次再挑选一个低优先的 Pod2 进行同样的过程，绑定到 Node1 上



## 优先级抢占

* Node1上防止了Pod0，同样有Pod1和Pod2待调度，Pod1高优先级，Pod2低优先级

* Pod2先进行调度，调度到Node1上
* 之后Pod1再进行调度，因为 Node1 上已经存在了两个 Pod，资源不足，所以会遇到调度失败

* 在调度失败时 Pod1 会进入抢占流程，这时会进行整个集群的节点筛选，最后挑出要抢占的 Pod 是 Pod2，此时调度器会把 Pod2 从 Node1 上移除
* 再把 Pod1 调度到 Node1 上。这样就完成了一次抢占调度的流程
